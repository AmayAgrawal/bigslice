#!/usr/bin/env python
# vim: set fileencoding=utf-8 :
#
# Copyright (C) 2019 Satria A. Kautsar
# Wageningen University & Research
# Bioinformatics Group
"""the main script of bigsscuit"""

import argparse
from os import path, makedirs, remove
from sys import exit, argv
import glob
from multiprocessing import Pool, cpu_count
from modules.data.database import Database
from modules.data.bgc import BGC
from modules.data.hmm import HMMDatabase
from modules.data.run import Run
from modules.data.hsp import HSP
from modules.utils import reversed_fp_iter
from hashlib import md5
from time import time
from datetime import datetime
import subprocess


_last_timestamp = None


def get_elapsed():
    # not so fancy, just for a quickie
    global _last_timestamp
    if not _last_timestamp:
        _last_timestamp = time()
        return 0
    else:
        now = time()
        elapsed = now - _last_timestamp
        _last_timestamp = now
        return elapsed


def run_hmmscan(arguments: tuple):
    fasta_path, hmm_path, result_path = arguments
    if path.exists(result_path):
        with open(result_path, "r") as fp:
            if next(reversed_fp_iter(fp)).rstrip() == "[ok]":
                # already hmmscanned
                return 1
            else:
                # file is broken, remove
                remove(result_path)

    command = [
        "hmmscan",
        "--acc",
        "--cpu", "1",
        "-o", result_path,
        hmm_path,
        fasta_path
    ]
    try:
        ret = subprocess.run(command, check=True)
        if ret.returncode != 0 and path.exists(result_path):
            remove(result_path)
        return ret.returncode == 0
    except subprocess.CalledProcessError as e:
        print("hmmscan error: {}".format(e.stderr))
        pass

    return False


def update_bgc_status(bgc_id, run_id, status, database):
    return database.update("run_bgc_status",
                           {"status": status},
                           "WHERE run_id=? AND bgc_id=?",
                           (run_id, bgc_id))


def subpfam_scan(arguments: tuple):
    # TODO: implement this
    return -1


def feature_extraction(arguments: tuple):
    # TODO: implement this
    return -1


def check_pfams_exist(bgc_id: int, pfam_ids: list, database: Database):
    # TODO: SQLite can't handle more than 1000 parameters
    # break down pfam_ids to manageable chunks
    params = [bgc_id]
    params.extend(pfam_ids)
    return [row["hmm_id"] for row in database.select(
        "hsp,cds",
        "WHERE hsp.cds_id=cds.id" +
        " AND cds.bgc_id=?" +
        " AND hsp.hmm_id IN (" + ",".join(["?" for i in pfam_ids]) + ")",
        parameters=tuple(params),
        props=["hsp.hmm_id"],
        distinct=True
    )]


def check_hmmscanned_in_db(bgc_id: int,
                           md5_biosyn_pfam: str, database: Database):
    for row in database.select(
        "run_bgc_status,run,hmm_db",
        "WHERE run_bgc_status.run_id=run.id AND " +
        "run.hmm_db_id=hmm_db.id AND " +
        "hmm_db.md5_biosyn_pfam=? AND " +
        "bgc_id=?",
        parameters=(md5_biosyn_pfam, bgc_id),
            props=["run_bgc_status.status as status"]):
        if row["status"] >= 2:
            return True
    return False


def check_subpfam_scanned_in_db(bgc_id: int,
                                md5_sub_pfam: str, database: Database):
    for row in database.select(
        "run_bgc_status,run,hmm_db",
        "WHERE run_bgc_status.run_id=run.id AND " +
        "run.hmm_db_id=hmm_db.id AND " +
        "hmm_db.md5_sub_pfam=? AND " +
        "bgc_id=?",
        parameters=(md5_sub_pfam, bgc_id),
            props=["run_bgc_status.status as status"]):
        if row["status"] >= 3:
            return True
    return False


def check_gbk_exists(file_path: str, database: Database):
    existing = database.select("bgc",
                               "WHERE orig_filename=?",
                               parameters=(path.basename(file_path),),
                               props=["id"])
    if len(existing) > 0:
        return [row["id"] for row in existing]
    else:
        return []


def process_input_folder(folder_path: str, database: Database,
                         mp_pool: Pool):

    # check if input gbks exist in database
    print("processing {}...".format(folder_path))
    get_elapsed()
    all_bgc_ids = set()
    files_to_process = []
    count_gbk_exists = 0
    for file_path in glob.iglob(path.join(folder_path, "*.gbk")):
        bgc_ids = check_gbk_exists(file_path, database)
        if len(bgc_ids) < 1:
            files_to_process.append(file_path)
        else:
            count_gbk_exists += 1
            all_bgc_ids.update(bgc_ids)
    print("Found {} BGCs from {} GBKs ({} is/are to be parsed).".format(
        len(all_bgc_ids), count_gbk_exists, len(files_to_process)))
    print("[{}s] querying GBKs".format(get_elapsed()))

    # parse new GBKs
    get_elapsed()
    print("Parsing and inserting {} GBKs...".format(len(files_to_process)))
    pool_results = mp_pool.map(parse_input_gbk, files_to_process)
    new_bgc_ids = []
    for file_path, bgcs in pool_results:
        for bgc in bgcs:
            bgc.save(database)
            new_bgc_ids.append(bgc.id)
    all_bgc_ids.update(new_bgc_ids)
    database.commit_inserts()
    print("Inserted {} new BGCs.".format(len(new_bgc_ids)))
    print("[{}s] inserting BGCs".format(get_elapsed()))

    return all_bgc_ids


def parse_input_gbk(file_path: str):
    return (file_path, BGC.parse_gbk(file_path))


def get_chunk(list_of_ids: list, chunk_size: int=100):
    i = 0
    while i * chunk_size <= len(list_of_ids):
        chunk = list_of_ids[
            i:min(i + chunk_size, len(list_of_ids) - 1)]
        chunk_name = md5(",".join(
            map(str, chunk)).encode('utf-8')).hexdigest()
        yield (chunk, chunk_name)
        i += 1


def main():

    # TODO: check requirements

    # program parameters
    parser = argparse.ArgumentParser(description="tbd")
    parser.add_argument("output_folder", type=str)
    parser.add_argument("-t", "--num_threads",
                        default=cpu_count(), type=int)
    parser.add_argument("-i", "--input_folder", type=str)
    parser.add_argument("--mem", action="store_true")
    parser.add_argument("--resume", action='store_true')
    parser.add_argument("--program_db_folder",
                        default=path.join(path.dirname(
                            path.realpath(__file__)), "db"), type=str)
    args = parser.parse_args()

    # define variables
    run_start = datetime.now()
    prog_params = " ".join(argv[1:])
    pool = Pool(processes=args.num_threads)
    output_folder = path.abspath(args.output_folder)
    tmp_folder = path.join(output_folder, "tmp")
    data_db_path = path.join(output_folder, "data.db")
    program_db_folder = path.abspath(args.program_db_folder)
    mibig_gbks_folder = path.join(program_db_folder, "mibig_gbks")
    resume = args.resume
    use_memory = args.mem

    # first gate keeping
    if resume and args.input_folder:
        print("--resume is selected but --input_folder is specified, " +
              "please select either!")
        return 1
    if args.input_folder:
        input_folder = path.abspath(args.input_folder)
        if not path.exists(input_folder):
            print("Can't find {}!".format(input_folder))
            return 1

    # create output folder if not exists
    if not path.exists(output_folder):
        makedirs(output_folder)
    else:
        if not resume:
            user_input = input("Folder " + output_folder +
                               " exists! continue running program (Y/[N])? ")
            if user_input not in ["Y", "y"]:
                print("Cancelled.")
                return 1
            del user_input

    # check if database backup exists
    if use_memory:
        db_backup_path = data_db_path + ".bak"
        if path.exists(db_backup_path):
            user_input = input("File " + db_backup_path +
                               " exists! it will get overwritten," +
                               " continue (Y/[N])?")
            if user_input not in ["Y", "y"]:
                print("Cancelled.")
                return 1
            del user_input
        del db_backup_path

    # load/create SQLite3 database
    get_elapsed()
    with Database(data_db_path, use_memory) as output_db:
        print("[{}s] loading sqlite3 database".format(get_elapsed()))

        # load HMM databases
        print("Loading HMM databases...")
        get_elapsed()
        hmm_db = HMMDatabase.load_folder(program_db_folder, output_db)
        md5_biosyn_pfam = hmm_db.md5_biosyn_pfam
        md5_sub_pfam = hmm_db.md5_sub_pfam
        if hmm_db.id < 1:
            hmm_db.save(output_db)
            output_db.commit_inserts()
        print("[{}s] loading hmm databases".format(get_elapsed()))

        if resume:
            # fetch latest run data
            run = Run.get_latest(hmm_db.id, output_db)
            if not run:
                print("No run data to resume, exiting..")
                return 1
            else:
                print("Resuming run #{} (started {})".format(
                    run.id,
                    run.time_start
                ))
        else:
            # read input folder
            all_bgc_ids = set()

            # first, parse and read MIBiGs
            # TODO: can this be pre-cached?
            all_bgc_ids.update(process_input_folder(
                mibig_gbks_folder, output_db, pool))

            # then the input folder
            all_bgc_ids.update(process_input_folder(
                input_folder, output_db, pool))

            # create new run data
            run = Run.create(all_bgc_ids, hmm_db.id, prog_params,
                             run_start, output_db)
            output_db.commit_inserts()

        # check run_bgc status, bin into a list of statuses
        get_elapsed()
        bgc_status_bin = {
            1: set(),  # RUN_STARTED
            2: set(),  # BIOSYN_SCANNED
            3: set(),  # SUBPFAM_SCANNED
            4: set(),  # FEATURES_EXTRACTED
            5: set(),  # CLUSTERING_FINISHED
            6: set()   # RUN_FINISHED
        }
        print("Checking run status of {} BGCs...".format(len(run.bgcs)))
        for bgc_id, bgc_status in run.bgcs.items():
            if bgc_status > run.status:
                print("bgc_id {} run_status is {} while the whole run is \
                    {}. the run data might be corrupted!".format(
                    bgc_id, bgc_status, run.status))
                return 1
            bgc_status_bin[bgc_status].add(bgc_id)
        print("[{}s] checking run status".format(get_elapsed()))

        # phase 2: HMM scanning (BIOSYNC_SCANNED)
        if len(bgc_status_bin[1]) > 0:
            print("Doing biosyn_pfam scan on {} BGCs...".format(
                len(bgc_status_bin[1])))
            get_elapsed()
            hmm_scanned = set()

            # check if already hmmscanned in previous run
            for bgc_id in bgc_status_bin[1]:  # hmm_scanned should be empty
                if check_hmmscanned_in_db(bgc_id, md5_biosyn_pfam, output_db):
                    if update_bgc_status(bgc_id, run.id, 2, output_db) == 1:
                        hmm_scanned.add(bgc_id)
                    else:
                        print("Failed to update bgc status " +
                              "(bgc_id: " + str(bgc_id) + ") " +
                              "(run_id: " + str(run.id) + ")")
                        return 1
            print("{} BGCs are already scanned in previous run".format(
                len(hmm_scanned)))

            # prepare for hmmscan
            biosyn_tmp_folder = path.join(tmp_folder, md5_biosyn_pfam)
            if not path.exists(biosyn_tmp_folder):
                makedirs(biosyn_tmp_folder)

            def fasta_path(bgc_id):
                return path.join(
                    biosyn_tmp_folder, "bgc_{}.fa".format(bgc_id))

            def hmm_result_path(bgc_id):
                return path.join(
                    biosyn_tmp_folder, "bgc_{}.hmmtxt".format(bgc_id))

            to_be_hmmscanned = list(bgc_status_bin[1] - hmm_scanned)
            if len(to_be_hmmscanned) > 0:

                print("Preparing fasta files for hmmscans...")
                # write down all CDSes multifasta to be hmmscanned
                for bgc_id in to_be_hmmscanned:
                    if not path.exists(fasta_path(bgc_id)):
                        with open(fasta_path(bgc_id), "w") as fa:
                            fa.write(BGC.get_all_cds_fasta(bgc_id, output_db))

                print("Running hmmscans in parallel...")
                # do hmmscans in parallel
                hmm_path = path.join(program_db_folder,
                                     "biosynthetic_pfams",
                                     "Pfam-A.biosynthetic.hmm")
                hmmscan_results = pool.map(run_hmmscan,
                                           [(fasta_path(bgc_id),
                                             hmm_path,
                                             hmm_result_path(bgc_id))
                                               for bgc_id in to_be_hmmscanned])

                print("Parsing hmmscans results...")
                # parse hmmscan txts and create database
                # entries for them, then update bgc_status
                hmm_ids = {
                    hmm.accession: hmm.id for hmm in hmm_db.biosyn_pfams}
                for i, bgc_id in enumerate(to_be_hmmscanned):
                    if hmmscan_results[i]:
                        for hsp_object in HSP.parse_hmmtext(
                                hmm_result_path(bgc_id), hmm_ids):
                            hsp_object.save(output_db)
                        if update_bgc_status(
                                bgc_id, run.id, 2, output_db) == 1:
                            output_db.commit_inserts()
                            hmm_scanned.add(bgc_id)
                            # clean up
                            remove(fasta_path(bgc_id))
                            remove(hmm_result_path(bgc_id))
                        else:
                            print("Failed to update bgc status " +
                                  "(bgc_id: " + str(bgc_id) + ") " +
                                  "(run_id: " + str(run.id) + ")")
                            return 1

                del hmm_ids
            del to_be_hmmscanned
            del biosyn_tmp_folder
            del fasta_path
            del hmm_result_path

            # update status bin
            for bgc_id in hmm_scanned:
                assert run.bgcs[bgc_id] == 1
                run.bgcs[bgc_id] = 2
            bgc_status_bin[1] = bgc_status_bin[1] - hmm_scanned
            bgc_status_bin[2].update(hmm_scanned)

            print("[{}s] biosyn_pfam scan".format(get_elapsed()))
        if len(bgc_status_bin[1]) == 0:
            if run.status < 2:
                if run.update_status(2) > 0:
                    print("run_status is now BIOSYN_SCANNED")
                else:
                    print("failed to update run_status")
        else:
            print("Encountered errors in hmmscans, " +
                  str(len(bgc_status_bin[1])) +
                  " BGCs are not yet scanned.")
            return 1

        # phase 3: Subpfam scanning
        if len(bgc_status_bin[2]) > 0:
            get_elapsed()
            print("Doing sub_pfam scan on {} BGCs...".format(
                len(bgc_status_bin[2])))

            subpfam_scanned = set()

            # check if already subpfam_scanned in previous run
            for bgc_id in bgc_status_bin[2]:
                if check_subpfam_scanned_in_db(
                        bgc_id, md5_sub_pfam, output_db):
                    if update_bgc_status(bgc_id, run.id, 3, output_db) == 1:
                        subpfam_scanned.add(bgc_id)
                    else:
                        print("Failed to update bgc status " +
                              "(bgc_id: " + str(bgc_id) + ") " +
                              "(run_id: " + str(run.id) + ")")
                        return 1
            print("{} BGCs are already scanned in previous run".format(
                len(subpfam_scanned)))

            # prepare for subpfam_scan
            subpfam_tmp_folder = path.join(tmp_folder, md5_sub_pfam)
            if not path.exists(subpfam_tmp_folder):
                makedirs(subpfam_tmp_folder)

            def fasta_path(bgcs_md5, hmm_id):
                return path.join(
                    subpfam_tmp_folder, "bgc_{}_hmm_{}.fa".format(
                        bgcs_md5, hmm_id))

            def hmm_result_path(bgcs_md5, hmm_id):
                return path.join(
                    subpfam_tmp_folder, "bgc_{}_hmm_{}.hmmtxt".format(
                        bgcs_md5, hmm_id))

            # fetch core pfam ids
            core_pfam_ids = {}
            core_pfam_accs = {}
            for parent_hmm_acc in hmm_db.sub_pfams:
                for hmm_obj in hmm_db.biosyn_pfams:
                    if hmm_obj.accession == parent_hmm_acc:
                        core_pfam_ids[hmm_obj.accession] = hmm_obj.id
                        core_pfam_accs[hmm_obj.id] = hmm_obj.accession
                        break

            to_be_subpfam_scanned = list(bgc_status_bin[2] - subpfam_scanned)
            if len(to_be_subpfam_scanned) > 0:

                # fetch list of bgcs and core pfam ids
                print("Querying list of BGCs to be subpfam_scanned...")
                bgc_core_pfams = {}
                for bgc_id in to_be_subpfam_scanned:
                    bgc_core_pfams[bgc_id] = check_pfams_exist(
                        bgc_id, core_pfam_ids.values(), output_db)

                print("Preparing fasta files for subpfam_scans...")
                hmmscan_queues = set()
                for chunk, chunk_name in get_chunk(to_be_subpfam_scanned):
                    fasta_sequences = {}
                    for bgc_id in chunk:
                        for parent_hmm_id in bgc_core_pfams[bgc_id]:
                            in_fasta_path = fasta_path(
                                chunk_name, parent_hmm_id)
                            hmm_path = path.join(program_db_folder,
                                                 "sub_pfams",
                                                 "hmm",
                                                 core_pfam_accs[
                                                     parent_hmm_id] +
                                                 ".subpfams.hmm")
                            out_result_path = hmm_result_path(
                                chunk_name, parent_hmm_id)

                            if not path.exists(in_fasta_path):
                                if in_fasta_path not in fasta_sequences:
                                    fasta_sequences[in_fasta_path] = ""
                                fasta_sequences[in_fasta_path] += \
                                    BGC.get_all_cds_fasta_with_hits(
                                    bgc_id, parent_hmm_id, output_db)

                            hmmscan_queues.add(
                                (in_fasta_path, hmm_path, out_result_path))

                    # write down all CDSes multifasta to be hmmscanned
                    for in_fasta_path, multifasta in fasta_sequences.items():
                        with open(in_fasta_path, "w") as fa:
                            fa.write(multifasta)

                # do hmmscans in parallel
                print("Running subpfam_scans in parallel...")
                hmmscan_results = pool.map(
                    run_hmmscan, hmmscan_queues)

                print("Parsing subpfam_scans results...")
                for chunk, chunk_name in get_chunk(to_be_subpfam_scanned):
                    chunk_parent_pfams = set()
                    for bgc_id in chunk:
                        chunk_parent_pfams.update(bgc_core_pfams[bgc_id])
                    for parent_hmm_id in chunk_parent_pfams:
                        out_result_path = hmm_result_path(
                            chunk_name, parent_hmm_id)
                        sub_pfam_ids = {
                            hmm_obj.name: hmm_obj.id
                            for hmm_obj in
                            hmm_db.sub_pfams[core_pfam_accs[parent_hmm_id]]
                        }

                        for hsp_object in HSP.parse_hmmtext(
                                out_result_path, sub_pfam_ids):
                            hsp_object.save(output_db)

                    output_db.commit_inserts()

                    for bgc_id in chunk:
                        if update_bgc_status(
                                bgc_id, run.id, 3, output_db) == 1:
                            subpfam_scanned.add(bgc_id)
                        else:
                            print("Failed to update bgc status " +
                                  "(bgc_id: " + str(bgc_id) + ") " +
                                  "(run_id: " + str(run.id) + ")")
                            return 1

                    # clean up
                    for parent_hmm_id in chunk_parent_pfams:
                        remove(fasta_path(
                            chunk_name, parent_hmm_id))
                        remove(hmm_result_path(
                            chunk_name, parent_hmm_id))

            # update status bin
            for bgc_id in subpfam_scanned:
                assert run.bgcs[bgc_id] == 2
                run.bgcs[bgc_id] = 3
            bgc_status_bin[2] = bgc_status_bin[1] - subpfam_scanned
            bgc_status_bin[3].update(subpfam_scanned)

            print("[{}s] sub_pfam scan".format(get_elapsed()))
            return 1
        if len(bgc_status_bin[2]) == 0:
            if run.status < 3:
                if run.update_status(3) > 0:
                    print("run_status is now SUBPFAM_SCANNED")
                else:
                    print("failed to update run_status")
        else:
            print("Encountered errors in hmmscans, " +
                  str(len(bgc_status_bin[2])) +
                  " BGCs are not yet scanned.")
            return 1

        # phase 4: Features extraction
        arguments = [bgc_run[0] for bgc_run
                     in run.bgcs if bgc_run[1] < 4]
        if len(arguments) > 0:
            get_elapsed()
            print("Extracting features from {} BGCs...".format(len(arguments)))
            reports = pool.map(feature_extraction, arguments)
            count_errors = reports.count(-1)
            if count_errors > 0:
                print("Failed to extract features from BGCs.".format(
                    count_errors))
                exit(1)
            print("[{}s] features extraction".format(get_elapsed()))
        if run.status < 4:
            run.status == 4
            run.save()

        # phase 5: Clustering
        exit(1)  # TODO: implements
        if run.status < 5:
            run.status == 5
            run.save()

        # phase 6: finishing, preparing outputs
        exit(1)  # TODO: implements
        if run.status < 6:
            run.status == 6
            run.save()

    # exit program cleanly
    return 0


if __name__ == "__main__":
    exit(main())
