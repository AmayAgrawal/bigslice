#!/usr/bin/env python
# vim: set fileencoding=utf-8 :
#
# Copyright (C) 2019 Satria A. Kautsar
# Wageningen University & Research
# Bioinformatics Group
"""the main script of bigsscuit"""

import argparse
from os import path, makedirs
from sys import exit, argv
import glob
import multiprocessing
from modules.data.database import Database
from modules.data.bgc import BGC
from modules.data.hmm import HMMDatabase
from modules.data.run import Run
from time import time
from datetime import datetime


_last_timestamp = None


def get_elapsed():
    # not so fancy, just for a quickie
    global _last_timestamp
    if not _last_timestamp:
        _last_timestamp = time()
        return 0
    else:
        now = time()
        elapsed = now - _last_timestamp
        _last_timestamp = now
        return elapsed


def check_run_bgc(arguments: tuple):
    bgc_id, status, run_status = arguments
    if status > run_status:
        return -1  # should not happen
    else:
        return 1


def biosyn_scan(arguments: tuple):
    # TODO: implement this
    return -1


def subpfam_scan(arguments: tuple):
    # TODO: implement this
    return -1


def feature_extraction(arguments: tuple):
    # TODO: implement this
    return -1


def parse_input_gbk(arguments: tuple):
    file_path, output_db = arguments
    return (file_path,
            [bgc.id for bgc in BGC.parse_gbk(file_path, output_db, True)])


def main():

    # TODO: check requirements

    # program parameters
    parser = argparse.ArgumentParser(description="tbd")
    parser.add_argument("output_folder", type=str)
    parser.add_argument("-t", "--num_threads",
                        default=multiprocessing.cpu_count(), type=int)
    parser.add_argument("-i", "--input_folder", type=str)
    parser.add_argument("--resume", action='store_true')
    parser.add_argument("--program_db_folder",
                        default=path.join(path.dirname(
                            path.realpath(__file__)), "db"), type=str)
    args = parser.parse_args()

    # define variables
    run_start = datetime.now()
    prog_params = " ".join(argv[1:])
    pool = multiprocessing.Pool(processes=args.num_threads)
    output_folder = path.abspath(args.output_folder)
    data_db_path = path.join(output_folder, "data.db")
    program_db_folder = path.abspath(args.program_db_folder)
    mibig_gbks_folder = path.join(program_db_folder, "mibig_gbks")
    resume = args.resume

    # first gate keeping
    if resume and args.input_folder:
        print("--resume is selected but --input_folder is specified, " +
              "please select either!")
        exit(1)
    if args.input_folder:
        input_folder = path.abspath(args.input_folder)
        if not path.exists(input_folder):
            print("Can't find {}!".format(input_folder))
            exit(1)

    # create output folder if not exists
    if not path.exists(output_folder):
        makedirs(output_folder)
    else:
        if not resume:
            print("Folder " + output_folder +
                  " exists! continue running program? (Y/[N])")
            user_input = input()
            if user_input not in ["Y", "y"]:
                print("Cancelled.")
                exit(0)

    # load/create SQLite3 database
    get_elapsed()
    output_db = Database(data_db_path)
    print("[{}s] loading sqlite3 database".format(get_elapsed()))

    # load HMM databases
    print("Loading HMM databases...")
    get_elapsed()
    hmm_db_id = HMMDatabase.load_folder(program_db_folder, output_db, True).id
    print("[{}s] loading hmm databases".format(get_elapsed()))

    if resume:
        # fetch latest run data
        run = Run.get_latest(hmm_db_id, output_db)
        if not run:
            print("No run data to resume, exiting..")
            exit(1)
        else:
            print("Resuming run #{} (started {})".format(
                run.id,
                run.time_start
            ))
    else:
        all_bgc_ids = set()

        # load and parse input gbks
        get_elapsed()
        arguments = [(file_path, output_db) for file_path
                     in glob.iglob(path.join(input_folder, "*.gbk"))]
        print("Parsing {} genbank files...".format(len(arguments)))
        for file_path, bgc_ids in pool.map(parse_input_gbk, arguments):
            if len(bgc_ids) < 1:
                print("Can't parse any BGC from " + file_path)
            all_bgc_ids.update(bgc_ids)
        print("Found {} BGCs.".format(len(all_bgc_ids)))
        print("[{}s] processing input folder GBKs".format(get_elapsed()))

        # append mibig gbks
        get_elapsed()
        arguments = [(file_path, output_db) for file_path
                     in glob.iglob(path.join(mibig_gbks_folder, "*.gbk"))]
        print("Registering {} MIBiG entries...".format(len(arguments)))
        for file_path, bgc_ids in pool.map(parse_input_gbk, arguments):
            if len(bgc_ids) < 1:
                print("Can't parse any BGC from " + file_path)
            all_bgc_ids.update(bgc_ids)
        print("[{}s] processing MIBiG GBKs".format(get_elapsed()))

        # insert new run data
        run = Run.create(all_bgc_ids, hmm_db_id, prog_params,
                         run_start, output_db, True)

    # first, check if there is any run_bgc_status ahead
    # of run status (this means an error somewhere in the run process)
    get_elapsed()
    arguments = [(bgc_run[0], bgc_run[1], run.status) for bgc_run
                 in run.bgcs]
    print("Checking run status of {} BGCs...".format(len(arguments)))
    reports = pool.map(check_run_bgc, arguments)
    count_errors = reports.count(-1)
    if count_errors > 0:
        print("{} BGCs run_status is ahead. Run data is corrupted?".format(
            count_errors))
        exit(1)
    print("[{}s] checking run status".format(get_elapsed()))

    # phase 2: HMM scanning (BIOSYNC_SCANNED)
    arguments = [bgc_run[0] for bgc_run
                 in run.bgcs if bgc_run[1] < 2]
    if len(arguments) > 0:
        get_elapsed()
        print("Doing biosyn_pfam scan on {} BGCs...".format(len(arguments)))
        reports = pool.map(biosyn_scan, arguments)
        count_errors = reports.count(-1)
        if count_errors > 0:
            print("Failed to do biosyn_pfam scan on {} BGCs.".format(
                count_errors))
            exit(1)
        print("[{}s] biosyn_pfam scan".format(get_elapsed()))
    if run.status < 2:
        run.status == 2
        run.save()

    # phase 3: Subpfam scanning
    arguments = [bgc_run[0] for bgc_run
                 in run.bgcs if bgc_run[1] < 3]
    if len(arguments) > 0:
        get_elapsed()
        print("Doing sub_pfam scan on {} BGCs...".format(len(arguments)))
        reports = pool.map(subpfam_scan, arguments)
        count_errors = reports.count(-1)
        if count_errors > 0:
            print("Failed to do sub_pfam scan on {} BGCs.".format(
                count_errors))
            exit(1)
        print("[{}s] sub_pfam scan".format(get_elapsed()))
    if run.status < 3:
        run.status == 3
        run.save()

    # phase 4: Features extraction
    arguments = [bgc_run[0] for bgc_run
                 in run.bgcs if bgc_run[1] < 4]
    if len(arguments) > 0:
        get_elapsed()
        print("Extracting features from {} BGCs...".format(len(arguments)))
        reports = pool.map(feature_extraction, arguments)
        count_errors = reports.count(-1)
        if count_errors > 0:
            print("Failed to extract features from BGCs.".format(
                count_errors))
            exit(1)
        print("[{}s] features extraction".format(get_elapsed()))
    if run.status < 4:
        run.status == 4
        run.save()

    # phase 5: Clustering
    exit(1)  # TODO: implements
    if run.status < 5:
        run.status == 5
        run.save()

    # phase 6: finishing, preparing outputs
    exit(1)  # TODO: implements
    if run.status < 6:
        run.status == 6
        run.save()


if __name__ == "__main__":
    main()
