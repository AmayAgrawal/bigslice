#!/usr/bin/env python
# vim: set fileencoding=utf-8 :
#
# Copyright (C) 2019 Satria A. Kautsar
# Wageningen University & Research
# Bioinformatics Group
"""the main script of bigsscuit"""

import argparse
from os import getpid, path, makedirs, remove
from sys import exit, argv
import glob
import re
from multiprocessing import Pool, cpu_count
from modules.data.database import Database
from modules.data.bgc import BGC
from modules.data.hmm import HMMDatabase
from modules.data.run import Run
from modules.data.hsp import HSP
from modules.data.features import Features
from modules.clustering.birch import BirchClustering
from modules.utils import reversed_fp_iter, get_chunk
from time import time
import subprocess
from typing import List


_last_timestamp = None


def get_elapsed():
    # not so fancy, just for a quickie
    global _last_timestamp
    if not _last_timestamp:
        _last_timestamp = time()
        return 0
    else:
        now = time()
        elapsed = now - _last_timestamp
        _last_timestamp = now
        return elapsed


def fetch_pool(num_threads: int):
    pool = Pool(processes=num_threads)

    try:
        # set cores for the multiprocessing pools
        all_cpu_ids = set()
        for i, p in enumerate(pool._pool):
            cpu_id = str(cpu_count() - (i % cpu_count()) - 1)
            subprocess.run(["taskset",
                            "-p", "-c",
                            cpu_id,
                            str(p.pid)], check=True)
            all_cpu_ids.add(cpu_id)

        # set core for the main python script
        subprocess.run(["taskset",
                        "-p", "-c",
                        ",".join(all_cpu_ids),
                        str(getpid())], check=True)

    except FileNotFoundError:
        pass  # running in OSX?

    return pool


def run_hmmscan(arguments: tuple):
    fasta_path, hmm_path, result_path, cutoff, make_alignment = arguments
    if path.exists(result_path):
        with open(result_path, "r") as fp:
            if next(reversed_fp_iter(fp)).rstrip() == "[ok]":
                # already hmmscanned
                return 1
            else:
                # file is broken, remove
                remove(result_path)

    command = [
        "hmmscan",
        "--acc",
        "--cpu", "1",
        "-o", result_path,
        hmm_path,
        fasta_path
    ]

    if not make_alignment:
        command.insert(1, "--noali")

    if cutoff == "ga":
        command.insert(1, "--cut_ga")
    elif isinstance(cutoff, float) or isinstance(cutoff, int):
        command = [command[0]] + [
            "--domT", str(cutoff),
            "--incdomT", str(cutoff)
        ] + command[1:]

    try:
        ret = subprocess.run(command, check=True)
        if ret.returncode != 0 and path.exists(result_path):
            remove(result_path)
        return ret.returncode == 0
    except subprocess.CalledProcessError as e:
        print("hmmscan error: {}".format(e.stderr))
        pass

    return False


def update_bgc_status(bgc_id, run_id, status, database):
    return database.update("run_bgc_status",
                           {"status": status},
                           "WHERE run_id=? AND bgc_id=?",
                           (run_id, bgc_id))


def subpfam_scan(arguments: tuple):
    # TODO: implement this
    return -1


def feature_extraction(arguments: tuple):
    # TODO: implement this
    return -1


def check_pfams_exist(pfam_ids: List[int], database: Database):
    result = {}
    for row in database.select(
        "hsp,cds",
        "WHERE hsp.cds_id=cds.id" +
        " AND hsp.hmm_id IN (" + ",".join(["?" for i in pfam_ids]) + ")",
        parameters=tuple(pfam_ids),
        props=["bgc_id", "hmm_id"],
        distinct=True
    ):
        if row["bgc_id"] not in result:
            result[row["bgc_id"]] = set()
        result[row["bgc_id"]].add(row["hmm_id"])
    return result


def check_hmmscanned_in_db(bgc_ids: List[int],
                           md5_biosyn_pfam: str, database: Database):
    rows = database.select(
        "run_bgc_status,run,hmm_db",
        "WHERE run_bgc_status.run_id=run.id AND " +
        "run.hmm_db_id=hmm_db.id AND " +
        "hmm_db.md5_biosyn_pfam=? AND " +
        "run_bgc_status.status>=2 AND " +
        "bgc_id IN (" + ",".join(map(str, bgc_ids)) + ")",
        parameters=(md5_biosyn_pfam,),
        props=["bgc_id"])

    return([row["bgc_id"] for row in rows])


def check_subpfam_scanned_in_db(bgc_ids: List[int],
                                md5_sub_pfam: str, database: Database):
    rows = database.select(
        "run_bgc_status,run,hmm_db",
        "WHERE run_bgc_status.run_id=run.id AND " +
        "run.hmm_db_id=hmm_db.id AND " +
        "hmm_db.md5_sub_pfam=? AND " +
        "run_bgc_status.status>=3 AND " +
        "bgc_id IN (" + ",".join(map(str, bgc_ids)) + ")",
        parameters=(md5_sub_pfam,),
        props=["bgc_id"])

    return([row["bgc_id"] for row in rows])


def check_gbk_exists(dataset_id: int, orig_gbk_path: str, database: Database):
    existing = database.select("bgc",
                               "WHERE dataset_id=? AND orig_gbk_path=?",
                               parameters=(dataset_id, orig_gbk_path),
                               props=["id"])
    if len(existing) > 0:
        return [row["id"] for row in existing]
    else:
        return []


def process_input_folder(folder_path: str, database: Database,
                         mp_pool: Pool):

    # load metadata file
    metadata_file = path.join(folder_path, "datasets.tsv")
    datasets = {}
    if not path.exists(metadata_file):
        raise Exception(
            "Can't find metadata file ({})".format(metadata_file))
    with open(metadata_file, "r") as metadata_handle:
        for line in metadata_handle:
            if line.startswith("#"):  # comments
                continue
            line = line.rstrip()
            dataset_name, dataset_path, dataset_desc = line.split("\t")

            # dataset name should be unique
            if dataset_name in datasets:
                raise Exception(
                    "Dataset name should be unique! " +
                    "duplicated name found: " + dataset_name)

            # check if dataset folder exists
            dataset_folder_path = path.join(
                folder_path,
                dataset_path)
            if not path.exists(dataset_folder_path):
                raise Exception(
                    "Can't find dataset folder: {}".format(
                        dataset_folder_path))

            datasets[dataset_name] = {
                "path": dataset_path, "desc": dataset_desc}

    # define eligible regexes for clustergbks
    eligible_regexes = [re.compile(rgx) for rgx in [
        "^BGC[0-9]{7}$",  # MIBiG
        "^.+\\.cluster[0-9]+$",  # antiSMASH4 clustergbks
        "^.+\\.region[0-9]+$",  # antiSMASH5 clustergbks
    ]]

    # process datasets
    dataset_bgc_ids = {}
    for dataset_name, dataset_meta in datasets.items():

        # This only store bgcs from gbk files
        # exist within the folder
        # e.g. if a dataset has been parsed before,
        # then a folder is deleted, the previous BGCs
        # won't be included here
        dataset_bgc_ids[dataset_name] = set()

        # check if dataset exists in database
        queried_dataset = database.select(
            "dataset",
            "WHERE name=?",
            parameters=(dataset_name,),
            props=["id"])
        assert len(queried_dataset) <= 1

        if len(queried_dataset) > 0:
            # dataset exists, use entries from db
            # note:
            # in the future, we would like to match
            # between what's in the database
            # and what's in the folder, and adjust
            # the included BGCs accordingly
            # i.e. if there's a new file to parse
            dataset_id = queried_dataset[0]["id"]
            dataset_bgc_ids[dataset_name] = [
                row["id"] for row in database.select(
                    "bgc",
                    "WHERE dataset_id=?",
                    parameters=(dataset_id,),
                    props=["id"])]
            print("Found {} BGCs in the database.".format(
                len(dataset_bgc_ids[dataset_name])))
            print("[{}s] processing dataset: {}".format(
                get_elapsed(), dataset_name))
            continue

        else:  # create a new dataset entry
            dataset_id = database.insert(
                "dataset",
                {
                    "name": dataset_name,
                    "orig_folder": dataset_meta["path"],
                    "description": dataset_meta["desc"]
                }
            )

        get_elapsed()
        print("processing dataset: {}...".format(dataset_name))
        new_bgcs_count = 0
        files_to_process = []
        count_gbk_exists = 0

        # fetch gbk files
        dataset_folder_path = path.join(
            folder_path,
            dataset_meta["path"])
        for gbk_full_path in glob.iglob(path.join(
                dataset_folder_path,
                "**/*.gbk"),
                recursive=True):

            # first check: only take antiSMASH/MIBiG-derived GBKs
            gbk_name = path.splitext(path.basename(gbk_full_path))[0]
            eligible_file = False
            for eligible_pattern in eligible_regexes:
                if eligible_pattern.match(gbk_name):
                    eligible_file = True
                    break

            # second check: see if already exists in db
            if eligible_file:
                gbk_path = gbk_full_path[len(dataset_folder_path) + 1:]
                # note: this check is turned off for now,
                # see above note
                # check_gbk_exists(dataset_id, gbk_path, database)
                bgc_ids = []
                if len(bgc_ids) < 1:
                    files_to_process.append((gbk_path, gbk_full_path))
                else:
                    count_gbk_exists += 1
                    dataset_bgc_ids[dataset_name].update(bgc_ids)

        print("Found {} BGCs from {} GBKs, another {} to be parsed.".format(
            len(dataset_bgc_ids[dataset_name]),
            count_gbk_exists,
            len(files_to_process)))

        # parse new GBKs
        print("Parsing and inserting {} GBKs...".format(len(files_to_process)))
        pool_results = mp_pool.map(parse_input_gbk, files_to_process)
        for file_path, bgcs in pool_results:
            for bgc in bgcs:
                bgc.save(dataset_id, database)
                new_bgcs_count += 1
                dataset_bgc_ids[dataset_name].add(bgc.id)
        database.commit_inserts()
        print("Inserted {} new BGCs.".format(new_bgcs_count))
        print("[{}s] processing dataset: {}".format(
            get_elapsed(), dataset_name))

    return dataset_bgc_ids


def parse_input_gbk(arguments: tuple):
    orig_gbk_path, file_path = arguments
    return (file_path, BGC.parse_gbk(file_path, orig_gbk_path=orig_gbk_path))


def main():

    # TODO: check requirements

    # program parameters
    parser = argparse.ArgumentParser(description="tbd")
    parser.add_argument("output_folder", type=str)
    parser.add_argument("-t", "--num_threads",
                        default=cpu_count(), type=int)
    parser.add_argument("--hmmscan_chunk_size",
                        default=100, type=int)
    parser.add_argument("--subpfam_chunk_size",
                        default=100, type=int)
    parser.add_argument("-i", "--input_folder", type=str)
    parser.add_argument("--threshold",
                        default=-1, type=float)
    parser.add_argument("--threshold_pct",
                        default=1.00, type=float)
    parser.add_argument("--mem", action="store_true")
    parser.add_argument("--resume", action='store_true')
    parser.add_argument("--program_db_folder",
                        default=path.join(path.dirname(
                            path.realpath(__file__)),
                            "db", "models"), type=str)
    args = parser.parse_args()

    # define variables
    prog_params = " ".join(argv[1:])
    pool = fetch_pool(args.num_threads)
    hmmscan_chunk_size = args.hmmscan_chunk_size
    subpfam_chunk_size = args.subpfam_chunk_size
    clustering_threshold = args.threshold
    clustering_threshold_percentile = args.threshold_pct
    output_folder = path.abspath(args.output_folder)
    features_folder = path.join(output_folder, "features")
    centroids_folder = path.join(output_folder, "centroids")
    tmp_folder = path.join(output_folder, "tmp")
    data_db_path = path.join(output_folder, "data.db")
    program_db_folder = path.abspath(args.program_db_folder)
    resume = args.resume
    use_memory = args.mem

    # first gate keeping
    if resume and args.input_folder:
        print("--resume is selected but --input_folder is specified, " +
              "please select either!")
        return 1
    if args.input_folder:
        input_folder = path.abspath(args.input_folder)
        if not path.exists(input_folder):
            print("Can't find {}!".format(input_folder))
            return 1

    # create output folder if not exists
    if not path.exists(output_folder):
        makedirs(output_folder)
    else:
        if not resume:
            user_input = input("Folder " + output_folder +
                               " exists! continue running program (Y/[N])? ")
            if user_input not in ["Y", "y"]:
                print("Cancelled.")
                return 1
            del user_input

    # check if database backup exists
    if use_memory:
        db_backup_path = data_db_path + ".bak"
        if path.exists(db_backup_path):
            user_input = input("File " + db_backup_path +
                               " exists! it will get overwritten," +
                               " continue (Y/[N])?")
            if user_input not in ["Y", "y"]:
                print("Cancelled.")
                return 1
            del user_input
        del db_backup_path

    # load/create SQLite3 database
    if use_memory:
        print("Loading database into memory (this can take a while)...")
    get_elapsed()
    with Database(data_db_path, use_memory) as output_db:
        print("[{}s] loading sqlite3 database".format(get_elapsed()))

        # load HMM databases
        print("Loading HMM databases...")
        get_elapsed()
        hmm_db = HMMDatabase.load_folder(program_db_folder, output_db)
        md5_biosyn_pfam = hmm_db.md5_biosyn_pfam
        md5_sub_pfam = hmm_db.md5_sub_pfam
        if hmm_db.id < 1:
            hmm_db.save(output_db)
            output_db.commit_inserts()
        print("[{}s] loading hmm databases".format(get_elapsed()))

        if resume:
            # fetch latest run data
            run = Run.get_latest(hmm_db.id, output_db)
            if not run:
                print("No run data to resume, exiting..")
                return 1
            else:
                print("Resuming run #{}".format(
                    run.id
                ))
                run.log("run resumed " + str(len(run.bgcs)) + " BGCs")
        else:
            # read input folder
            all_bgc_ids = set()
            datasets_count = 0

            for dataset_name, dataset_bgc_ids in process_input_folder(
                    input_folder, output_db, pool).items():
                datasets_count += 1
                all_bgc_ids.update(dataset_bgc_ids)
            print("Found {} BGC(s) from {} dataset(s)".format(
                len(all_bgc_ids),
                datasets_count))

            # create new run data
            run = Run.create(all_bgc_ids, hmm_db.id, prog_params, output_db)
            output_db.commit_inserts()
            run.log("run created " + str(len(run.bgcs)) + " BGCs")

            if use_memory:
                output_db.dump_db_file()

        # check run_bgc status, bin into a list of statuses
        get_elapsed()
        bgc_status_bin = {
            1: set(),  # RUN_STARTED
            2: set(),  # BIOSYN_SCANNED
            3: set(),  # SUBPFAM_SCANNED
            4: set(),  # FEATURES_EXTRACTED
            5: set(),  # CLUSTERING_FINISHED
            6: set()   # RUN_FINISHED
        }
        print("Checking run status of {} BGCs...".format(len(run.bgcs)))
        for bgc_id, bgc_status in run.bgcs.items():
            if bgc_status > run.status:
                print("bgc_id {} run_status is {} while the whole run is \
                    {}. the run data might be corrupted!".format(
                    bgc_id, bgc_status, run.status))
                return 1
            bgc_status_bin[bgc_status].add(bgc_id)
        print("[{}s] checking run status".format(get_elapsed()))

        # phase 2: HMM scanning (BIOSYNC_SCANNED)
        if len(bgc_status_bin[1]) > 0:
            print("Doing biosyn_pfam scan on {} BGCs...".format(
                len(bgc_status_bin[1])))
            run.log("biosyn_pfam start " +
                    str(len(bgc_status_bin[1])) + " BGCs")
            get_elapsed()
            hmm_scanned = set()

            # check if already hmmscanned in previous run
            for chunk, chunk_name in get_chunk(list(bgc_status_bin[1]), args.num_threads, 1000):
                for bgc_id in check_hmmscanned_in_db(
                        chunk, md5_biosyn_pfam, output_db):
                    if update_bgc_status(bgc_id, run.id, 2, output_db) == 1:
                        hmm_scanned.add(bgc_id)
                    else:
                        print("Failed to update bgc status " +
                              "(bgc_id: " + str(bgc_id) + ") " +
                              "(run_id: " + str(run.id) + ")")
                        return 1

            print("{} BGCs are already scanned in previous run".format(
                len(hmm_scanned)))

            # prepare for hmmscan
            biosyn_tmp_folder = path.join(tmp_folder, md5_biosyn_pfam)
            if not path.exists(biosyn_tmp_folder):
                makedirs(biosyn_tmp_folder)

            def fasta_path(chunk_name):
                return path.join(
                    biosyn_tmp_folder, "bgc_{}.fa".format(chunk_name))

            def hmm_result_path(chunk_name):
                return path.join(
                    biosyn_tmp_folder, "bgc_{}.hmmtxt".format(chunk_name))

            to_be_hmmscanned = list(bgc_status_bin[1] - hmm_scanned)
            if len(to_be_hmmscanned) > 0:

                print("Preparing fasta files for hmmscans...")
                run.log("hmmscan prepare " +
                        str(len(to_be_hmmscanned)) + " BGCs")

                hmmscan_queues = set()
                hmm_path = path.join(program_db_folder,
                                     "biosynthetic_pfams",
                                     "Pfam-A.biosynthetic.hmm")
                for chunk, chunk_name in get_chunk(
                        to_be_hmmscanned,
                        args.num_threads,
                        hmmscan_chunk_size):

                    fasta_sequences = BGC.get_all_cds_fasta(
                        chunk, output_db)

                    # write down all CDSes multifasta to be hmmscanned
                    in_fasta_path = fasta_path(chunk_name)
                    out_result_path = hmm_result_path(chunk_name)

                    if not path.exists(in_fasta_path):
                        with open(in_fasta_path, "w") as fa:
                            fa.write(fasta_sequences)
                            hmmscan_queues.add(
                                (in_fasta_path, hmm_path,
                                    out_result_path, "ga", True))

                print("Running hmmscans in parallel...")
                run.log("hmmscan run " +
                        str(len(to_be_hmmscanned)) + " BGCs")

                # do hmmscans in parallel
                _ = pool.map(
                    run_hmmscan, hmmscan_queues)

                print("Parsing hmmscans results...")
                run.log("hmmscan parse " +
                        str(len(to_be_hmmscanned)) + " BGCs")
                # parse hmmscan txts and create database
                # entries for them, then update bgc_status
                hmm_ids = {
                    hmm.accession: hmm.id for hmm in hmm_db.biosyn_pfams}

                for chunk, chunk_name in get_chunk(
                        to_be_hmmscanned,
                        args.num_threads,
                        hmmscan_chunk_size):

                    in_fasta_path = fasta_path(chunk_name)
                    out_result_path = hmm_result_path(chunk_name)

                    for hsp_object in HSP.parse_hmmtext(
                            out_result_path, hmm_ids):
                        hsp_object.save(output_db)

                    output_db.commit_inserts()

                    for bgc_id in chunk:
                        if update_bgc_status(
                                bgc_id, run.id, 2, output_db) == 1:
                            hmm_scanned.add(bgc_id)
                        else:
                            print("Failed to update bgc status " +
                                  "(bgc_id: " + str(bgc_id) + ") " +
                                  "(run_id: " + str(run.id) + ")")

                    # clean up
                    remove(in_fasta_path)
                    remove(out_result_path)

                del hmm_ids

            # update status bin
            for bgc_id in hmm_scanned:
                assert run.bgcs[bgc_id] == 1
                run.bgcs[bgc_id] = 2
            bgc_status_bin[1] = bgc_status_bin[1] - hmm_scanned
            bgc_status_bin[2].update(hmm_scanned)

            if len(to_be_hmmscanned) > 0 and use_memory:
                output_db.dump_db_file()

            # garbage collection
            del to_be_hmmscanned
            del biosyn_tmp_folder
            del fasta_path
            del hmm_result_path

            print("[{}s] biosyn_pfam scan".format(get_elapsed()))
            run.log("biosyn_pfam end")
        if len(bgc_status_bin[1]) == 0:
            if run.status < 2:
                if run.update_status(2) > 0:
                    print("run_status is now BIOSYN_SCANNED")
                else:
                    print("failed to update run_status")
        else:
            print("Encountered errors in hmmscans, " +
                  str(len(bgc_status_bin[1])) +
                  " BGCs are not yet scanned.")
            return 1

        # phase 3: Subpfam scanning
        if len(bgc_status_bin[2]) > 0:
            get_elapsed()
            print("Doing sub_pfam scan on {} BGCs...".format(
                len(bgc_status_bin[2])))
            run.log("sub_pfam start " +
                    str(len(bgc_status_bin[2])) + " BGCs")

            subpfam_scanned = set()

            # check if already subpfam_scanned in previous run
            for chunk, chunk_name in get_chunk(list(bgc_status_bin[2]), args.num_threads, subpfam_chunk_size):
                for bgc_id in check_subpfam_scanned_in_db(
                        chunk, md5_sub_pfam, output_db):
                    if update_bgc_status(bgc_id, run.id, 3, output_db) == 1:
                        subpfam_scanned.add(bgc_id)
                    else:
                        print("Failed to update bgc status " +
                              "(bgc_id: " + str(bgc_id) + ") " +
                              "(run_id: " + str(run.id) + ")")
                        return 1

            print("{} BGCs are already scanned in previous run".format(
                len(subpfam_scanned)))

            # prepare for subpfam_scan
            subpfam_tmp_folder = path.join(tmp_folder, md5_sub_pfam)
            if not path.exists(subpfam_tmp_folder):
                makedirs(subpfam_tmp_folder)

            def fasta_path(bgcs_md5, hmm_id):
                return path.join(
                    subpfam_tmp_folder, "bgc_{}_hmm_{}.fa".format(
                        bgcs_md5, hmm_id))

            def hmm_result_path(bgcs_md5, hmm_id):
                return path.join(
                    subpfam_tmp_folder, "bgc_{}_hmm_{}.hmmtxt".format(
                        bgcs_md5, hmm_id))

            # fetch core pfam ids
            core_pfam_ids = {}
            core_pfam_accs = {}
            for parent_hmm_acc in hmm_db.sub_pfams:
                for hmm_obj in hmm_db.biosyn_pfams:
                    if hmm_obj.accession == parent_hmm_acc:
                        core_pfam_ids[hmm_obj.accession] = hmm_obj.id
                        core_pfam_accs[hmm_obj.id] = hmm_obj.accession
                        break

            to_be_subpfam_scanned = list(bgc_status_bin[2] - subpfam_scanned)
            if len(to_be_subpfam_scanned) > 0:

                chunks_and_pfam_ids = {}

                print("Preparing fasta files for subpfam_scans...")
                run.log("hmmscan prepare " +
                        str(len(to_be_subpfam_scanned)) + " BGCs")
                hmmscan_queues = set()
                for chunk, chunk_name in get_chunk(
                        to_be_subpfam_scanned,
                        args.num_threads,
                        subpfam_chunk_size):

                    fasta_sequences = BGC.get_all_aligned_hsp(
                        chunk, core_pfam_ids.values(), output_db)
                    chunks_and_pfam_ids[chunk_name] = set(
                        fasta_sequences.keys())

                    for parent_hmm_id in fasta_sequences:
                        in_fasta_path = fasta_path(
                            chunk_name, parent_hmm_id)
                        hmm_path = path.join(program_db_folder,
                                             "sub_pfams",
                                             "hmm",
                                             core_pfam_accs[
                                                 parent_hmm_id] +
                                             ".subpfams.hmm")
                        out_result_path = hmm_result_path(
                            chunk_name, parent_hmm_id)

                        if not path.exists(in_fasta_path):
                            with open(in_fasta_path, "w") as fa:
                                fa.write(fasta_sequences[parent_hmm_id])

                        hmmscan_queues.add(
                            (in_fasta_path,
                                hmm_path,
                                out_result_path,
                                20,  # for subpfam_scan
                                False
                             ))

                # do hmmscans in parallel
                print("Running subpfam_scans in parallel...")
                run.log("hmmscan run " +
                        str(len(to_be_subpfam_scanned)) + " BGCs")
                _ = pool.map(
                    run_hmmscan, hmmscan_queues)

                print("Parsing subpfam_scans results...")
                run.log("hmmscan parse " +
                        str(len(to_be_subpfam_scanned)) + " BGCs")
                for chunk, chunk_name in get_chunk(
                        to_be_subpfam_scanned,
                        args.num_threads,
                        subpfam_chunk_size):
                    chunk_parent_pfams = chunks_and_pfam_ids[chunk_name]
                    for parent_hmm_id in chunk_parent_pfams:
                        out_result_path = hmm_result_path(
                            chunk_name, parent_hmm_id)
                        sub_pfam_ids = {
                            hmm_obj.name: hmm_obj.id
                            for hmm_obj in
                            hmm_db.sub_pfams[core_pfam_accs[parent_hmm_id]]
                        }

                        for hsp_object in HSP.parse_hmmtext(
                                out_result_path, sub_pfam_ids,
                                save_alignment=False,
                                top_k=3,
                                rank_normalize=True):
                            hsp_object.save(output_db)

                    output_db.commit_inserts()

                    for bgc_id in chunk:
                        if update_bgc_status(
                                bgc_id, run.id, 3, output_db) == 1:
                            subpfam_scanned.add(bgc_id)
                        else:
                            print("Failed to update bgc status " +
                                  "(bgc_id: " + str(bgc_id) + ") " +
                                  "(run_id: " + str(run.id) + ")")

                    # clean up
                    for parent_hmm_id in chunk_parent_pfams:
                        remove(fasta_path(
                            chunk_name, parent_hmm_id))
                        remove(hmm_result_path(
                            chunk_name, parent_hmm_id))

            # update status bin
            for bgc_id in subpfam_scanned:
                assert run.bgcs[bgc_id] == 2
                run.bgcs[bgc_id] = 3
            bgc_status_bin[2] = bgc_status_bin[1] - subpfam_scanned
            bgc_status_bin[3].update(subpfam_scanned)

            print("[{}s] sub_pfam scan".format(get_elapsed()))
            run.log("sub_pfam end")

            if len(to_be_subpfam_scanned) > 0 and use_memory:
                output_db.dump_db_file()

        if len(bgc_status_bin[2]) == 0:
            if run.status < 3:
                if run.update_status(3) > 0:
                    print("run_status is now SUBPFAM_SCANNED")
                else:
                    print("failed to update run_status")
        else:
            print("Encountered errors in hmmscans, " +
                  str(len(bgc_status_bin[2])) +
                  " BGCs are not yet scanned.")
            return 1

        # phase 4: Features extraction
        if 2 < run.status < 4:
            get_elapsed()
            print("Extracting features from {} BGCs...".format(len(run.bgcs)))
            run.log("features_extraction start " +
                    str(len(run.bgcs)) + " BGCs")

            if not path.exists(features_folder):
                makedirs(features_folder)

            features = Features.extract(
                run.id, "default", output_db, pool
            )
            features.save(features_folder)
            del features

            print("[{}s] features extraction".format(get_elapsed()))
            run.log("features_extraction end")
            update_bgcs_status = output_db.update("run_bgc_status",
                                                  {"status": 4},
                                                  "WHERE run_id=?",
                                                  (run.id,))
            if update_bgcs_status > 0 and run.update_status(4) > 0:
                print("run_status is now FEATURES_EXTRACTED")
            else:
                print("failed to update run_status")

        # phase 5: Clustering
        if 3 < run.status < 5:
            get_elapsed()
            print("Clustering {} BGCs...".format(len(run.bgcs)))
            run.log("clustering start " +
                    str(len(run.bgcs)) + " BGCs")

            if not path.exists(centroids_folder):
                makedirs(centroids_folder)

            clustering = BirchClustering.run(
                run.id,
                features_folder,
                output_db,
                threshold=clustering_threshold,
                threshold_percentile=clustering_threshold_percentile
            )
            clustering.save(centroids_folder)
            del clustering

            print("[{}s] clustering".format(get_elapsed()))
            run.log("clustering end")
            update_bgcs_status = output_db.update("run_bgc_status",
                                                  {"status": 5},
                                                  "WHERE run_id=?",
                                                  (run.id,))
            if update_bgcs_status > 0 and run.update_status(5) > 0:
                print("run_status is now CLUSTERING_FINISHED")
            else:
                print("failed to update run_status")

        # phase 6: finishing, preparing outputs
        return 1  # TODO: implements
        if run.status < 6:
            run.status == 6
            run.save()

        run.log("run finished")
    # exit program cleanly
    return 0


if __name__ == "__main__":
    return_code = main()
    if return_code == 0:
        print("BiG-SLiCE run complete!")
    else:
        print("BiG-SLiCE run failed.")
