#!/usr/bin/env python
# vim: set fileencoding=utf-8 :
#
# Copyright (C) 2019 Satria A. Kautsar
# Wageningen University & Research
# Bioinformatics Group
"""the main script of bigsscuit"""

import argparse
from os import getpid, path, makedirs, remove
from sys import exit, argv
import glob
from multiprocessing import Pool, cpu_count
from modules.data.database import Database
from modules.data.bgc import BGC
from modules.data.hmm import HMMDatabase
from modules.data.run import Run
from modules.data.hsp import HSP
from modules.data.features import Features
from modules.clustering.birch import BirchClustering
from modules.utils import reversed_fp_iter, get_chunk
from time import time
from datetime import datetime
import subprocess
from typing import List


_last_timestamp = None


def get_elapsed():
    # not so fancy, just for a quickie
    global _last_timestamp
    if not _last_timestamp:
        _last_timestamp = time()
        return 0
    else:
        now = time()
        elapsed = now - _last_timestamp
        _last_timestamp = now
        return elapsed


def fetch_pool(num_threads: int):
    pool = Pool(processes=num_threads)

    try:
        # set cores for the multiprocessing pools
        all_cpu_ids = set()
        for i, p in enumerate(pool._pool):
            cpu_id = str(cpu_count() - (i % cpu_count()) - 1)
            subprocess.run(["taskset",
                            "-p", "-c",
                            cpu_id,
                            str(p.pid)], check=True)
            all_cpu_ids.add(cpu_id)

        # set core for the main python script
        subprocess.run(["taskset",
                        "-p", "-c",
                        ",".join(all_cpu_ids),
                        str(getpid())], check=True)

    except FileNotFoundError:
        pass  # running in OSX?

    return pool


def run_hmmscan(arguments: tuple):
    fasta_path, hmm_path, result_path, cutoff = arguments
    if path.exists(result_path):
        with open(result_path, "r") as fp:
            if next(reversed_fp_iter(fp)).rstrip() == "[ok]":
                # already hmmscanned
                return 1
            else:
                # file is broken, remove
                remove(result_path)

    command = [
        "hmmscan",
        "--acc",
        "--cpu", "1",
        "-o", result_path,
        hmm_path,
        fasta_path
    ]

    if cutoff == "ga":
        command.insert(1, "--cut_ga")
    elif isinstance(cutoff, float) or isinstance(cutoff, int):
        command = [command[0]] + [
            "--domT", str(cutoff),
            "--incdomT", str(cutoff)
        ] + command[1:]

    try:
        ret = subprocess.run(command, check=True)
        if ret.returncode != 0 and path.exists(result_path):
            remove(result_path)
        return ret.returncode == 0
    except subprocess.CalledProcessError as e:
        print("hmmscan error: {}".format(e.stderr))
        pass

    return False


def update_bgc_status(bgc_id, run_id, status, database):
    return database.update("run_bgc_status",
                           {"status": status},
                           "WHERE run_id=? AND bgc_id=?",
                           (run_id, bgc_id))


def subpfam_scan(arguments: tuple):
    # TODO: implement this
    return -1


def feature_extraction(arguments: tuple):
    # TODO: implement this
    return -1


def check_pfams_exist(pfam_ids: List[int], database: Database):
    result = {}
    for row in database.select(
        "hsp,cds",
        "WHERE hsp.cds_id=cds.id" +
        " AND hsp.hmm_id IN (" + ",".join(["?" for i in pfam_ids]) + ")",
        parameters=tuple(pfam_ids),
        props=["bgc_id", "hmm_id"],
        distinct=True
    ):
        if row["bgc_id"] not in result:
            result[row["bgc_id"]] = set()
        result[row["bgc_id"]].add(row["hmm_id"])
    return result


def check_hmmscanned_in_db(bgc_ids: List[int],
                           md5_biosyn_pfam: str, database: Database):
    rows = database.select(
        "run_bgc_status,run,hmm_db",
        "WHERE run_bgc_status.run_id=run.id AND " +
        "run.hmm_db_id=hmm_db.id AND " +
        "hmm_db.md5_biosyn_pfam=? AND " +
        "run_bgc_status.status>=2 AND " +
        "bgc_id IN (" + ",".join(map(str, bgc_ids)) + ")",
        parameters=(md5_biosyn_pfam,),
        props=["bgc_id"])

    return([row["bgc_id"] for row in rows])


def check_subpfam_scanned_in_db(bgc_ids: List[int],
                                md5_sub_pfam: str, database: Database):
    rows = database.select(
        "run_bgc_status,run,hmm_db",
        "WHERE run_bgc_status.run_id=run.id AND " +
        "run.hmm_db_id=hmm_db.id AND " +
        "hmm_db.md5_sub_pfam=? AND " +
        "run_bgc_status.status>=3 AND " +
        "bgc_id IN (" + ",".join(map(str, bgc_ids)) + ")",
        parameters=(md5_sub_pfam,),
        props=["bgc_id"])

    return([row["bgc_id"] for row in rows])


def check_gbk_exists(file_path: str, database: Database):
    existing = database.select("bgc",
                               "WHERE orig_filename=?",
                               parameters=(path.basename(file_path),),
                               props=["id"])
    if len(existing) > 0:
        return [row["id"] for row in existing]
    else:
        return []


def process_input_folder(folder_path: str, database: Database,
                         mp_pool: Pool):

    # check if input gbks exist in database
    print("processing {}...".format(folder_path))
    get_elapsed()
    all_bgc_ids = set()
    new_bgcs_count = 0
    files_to_process = []
    count_gbk_exists = 0
    for file_path in glob.iglob(path.join(folder_path, "*.gbk")):
        bgc_ids = check_gbk_exists(file_path, database)
        if len(bgc_ids) < 1:
            files_to_process.append(file_path)
        else:
            count_gbk_exists += 1
            all_bgc_ids.update(bgc_ids)
    print("Found {} BGCs from {} GBKs, another {} to be parsed.".format(
        len(all_bgc_ids), count_gbk_exists, len(files_to_process)))
    print("[{}s] querying GBKs".format(get_elapsed()))

    # parse new GBKs
    get_elapsed()
    print("Parsing and inserting {} GBKs...".format(len(files_to_process)))
    pool_results = mp_pool.map(parse_input_gbk, files_to_process)
    new_bgc_ids = []
    for file_path, bgcs in pool_results:
        for bgc in bgcs:
            bgc.save(database)
            new_bgc_ids.append(bgc.id)
            new_bgcs_count += 1
    all_bgc_ids.update(new_bgc_ids)
    database.commit_inserts()
    print("Inserted {} new BGCs.".format(len(new_bgc_ids)))
    print("[{}s] inserting BGCs".format(get_elapsed()))

    return all_bgc_ids, new_bgcs_count


def parse_input_gbk(file_path: str):
    return (file_path, BGC.parse_gbk(file_path))


def main():

    # TODO: check requirements

    # program parameters
    parser = argparse.ArgumentParser(description="tbd")
    parser.add_argument("output_folder", type=str)
    parser.add_argument("-t", "--num_threads",
                        default=cpu_count(), type=int)
    parser.add_argument("--hmmscan_chunk_size",
                        default=100, type=int)
    parser.add_argument("--subpfam_chunk_size",
                        default=100, type=int)
    parser.add_argument("-i", "--input_folder", type=str)
    parser.add_argument("--mem", action="store_true")
    parser.add_argument("--resume", action='store_true')
    parser.add_argument("--program_db_folder",
                        default=path.join(path.dirname(
                            path.realpath(__file__)), "db"), type=str)
    args = parser.parse_args()

    # define variables
    run_start = datetime.now()
    prog_params = " ".join(argv[1:])
    pool = fetch_pool(args.num_threads)
    hmmscan_chunk_size = args.hmmscan_chunk_size
    subpfam_chunk_size = args.subpfam_chunk_size
    output_folder = path.abspath(args.output_folder)
    features_folder = path.join(output_folder, "features")
    centroids_folder = path.join(output_folder, "centroids")
    tmp_folder = path.join(output_folder, "tmp")
    data_db_path = path.join(output_folder, "data.db")
    program_db_folder = path.abspath(args.program_db_folder)
    mibig_gbks_folder = path.join(program_db_folder, "mibig_gbks")
    resume = args.resume
    use_memory = args.mem

    # first gate keeping
    if resume and args.input_folder:
        print("--resume is selected but --input_folder is specified, " +
              "please select either!")
        return 1
    if args.input_folder:
        input_folder = path.abspath(args.input_folder)
        if not path.exists(input_folder):
            print("Can't find {}!".format(input_folder))
            return 1

    # create output folder if not exists
    if not path.exists(output_folder):
        makedirs(output_folder)
    else:
        if not resume:
            user_input = input("Folder " + output_folder +
                               " exists! continue running program (Y/[N])? ")
            if user_input not in ["Y", "y"]:
                print("Cancelled.")
                return 1
            del user_input

    # check if database backup exists
    if use_memory:
        db_backup_path = data_db_path + ".bak"
        if path.exists(db_backup_path):
            user_input = input("File " + db_backup_path +
                               " exists! it will get overwritten," +
                               " continue (Y/[N])?")
            if user_input not in ["Y", "y"]:
                print("Cancelled.")
                return 1
            del user_input
        del db_backup_path

    # load/create SQLite3 database
    if use_memory:
        print("Loading database into memory (this can take a while)...")
    get_elapsed()
    with Database(data_db_path, use_memory) as output_db:
        print("[{}s] loading sqlite3 database".format(get_elapsed()))

        # load HMM databases
        print("Loading HMM databases...")
        get_elapsed()
        hmm_db = HMMDatabase.load_folder(program_db_folder, output_db)
        md5_biosyn_pfam = hmm_db.md5_biosyn_pfam
        md5_sub_pfam = hmm_db.md5_sub_pfam
        if hmm_db.id < 1:
            hmm_db.save(output_db)
            output_db.commit_inserts()
        print("[{}s] loading hmm databases".format(get_elapsed()))

        if resume:
            # fetch latest run data
            run = Run.get_latest(hmm_db.id, output_db)
            if not run:
                print("No run data to resume, exiting..")
                return 1
            else:
                print("Resuming run #{} (started {})".format(
                    run.id,
                    run.time_start
                ))
        else:
            # read input folder
            all_bgc_ids = set()

            new_bgc_count = 0

            # first, parse and read MIBiGs
            # TODO: can this be pre-cached?
            mibig_bgc_ids, count_new = process_input_folder(
                mibig_gbks_folder, output_db, pool)
            all_bgc_ids.update(mibig_bgc_ids)
            new_bgc_count += count_new
            print("Found {} new BGC(s) from MIBiG".format(count_new))

            # then the input folder
            dataset_bgc_ids, count_new = process_input_folder(
                input_folder, output_db, pool)
            all_bgc_ids.update(dataset_bgc_ids)
            new_bgc_count += count_new
            print("Found {} new BGC(s) from input folder".format(count_new))

            # create new run data
            run = Run.create(all_bgc_ids, hmm_db.id, prog_params,
                             run_start, output_db)
            output_db.commit_inserts()

            if use_memory and new_bgc_count > 0:
                output_db.dump_db_file()

        # check run_bgc status, bin into a list of statuses
        get_elapsed()
        bgc_status_bin = {
            1: set(),  # RUN_STARTED
            2: set(),  # BIOSYN_SCANNED
            3: set(),  # SUBPFAM_SCANNED
            4: set(),  # FEATURES_EXTRACTED
            5: set(),  # CLUSTERING_FINISHED
            6: set()   # RUN_FINISHED
        }
        print("Checking run status of {} BGCs...".format(len(run.bgcs)))
        for bgc_id, bgc_status in run.bgcs.items():
            if bgc_status > run.status:
                print("bgc_id {} run_status is {} while the whole run is \
                    {}. the run data might be corrupted!".format(
                    bgc_id, bgc_status, run.status))
                return 1
            bgc_status_bin[bgc_status].add(bgc_id)
        print("[{}s] checking run status".format(get_elapsed()))

        # phase 2: HMM scanning (BIOSYNC_SCANNED)
        if len(bgc_status_bin[1]) > 0:
            print("Doing biosyn_pfam scan on {} BGCs...".format(
                len(bgc_status_bin[1])))
            get_elapsed()
            hmm_scanned = set()

            # check if already hmmscanned in previous run
            for chunk, chunk_name in get_chunk(list(bgc_status_bin[1]), 1000):
                for bgc_id in check_hmmscanned_in_db(
                        chunk, md5_biosyn_pfam, output_db):
                    if update_bgc_status(bgc_id, run.id, 2, output_db) == 1:
                        hmm_scanned.add(bgc_id)
                    else:
                        print("Failed to update bgc status " +
                              "(bgc_id: " + str(bgc_id) + ") " +
                              "(run_id: " + str(run.id) + ")")
                        return 1

            print("{} BGCs are already scanned in previous run".format(
                len(hmm_scanned)))

            # prepare for hmmscan
            biosyn_tmp_folder = path.join(tmp_folder, md5_biosyn_pfam)
            if not path.exists(biosyn_tmp_folder):
                makedirs(biosyn_tmp_folder)

            def fasta_path(chunk_name):
                return path.join(
                    biosyn_tmp_folder, "bgc_{}.fa".format(chunk_name))

            def hmm_result_path(chunk_name):
                return path.join(
                    biosyn_tmp_folder, "bgc_{}.hmmtxt".format(chunk_name))

            to_be_hmmscanned = list(bgc_status_bin[1] - hmm_scanned)
            if len(to_be_hmmscanned) > 0:

                print("Preparing fasta files for hmmscans...")

                hmmscan_queues = set()
                hmm_path = path.join(program_db_folder,
                                     "biosynthetic_pfams",
                                     "Pfam-A.biosynthetic.hmm")
                for chunk, chunk_name in get_chunk(
                        to_be_hmmscanned, hmmscan_chunk_size):

                    fasta_sequences = BGC.get_all_cds_fasta(
                        chunk, output_db)

                    # write down all CDSes multifasta to be hmmscanned
                    in_fasta_path = fasta_path(chunk_name)
                    out_result_path = hmm_result_path(chunk_name)

                    if not path.exists(in_fasta_path):
                        with open(in_fasta_path, "w") as fa:
                            fa.write(fasta_sequences)
                            hmmscan_queues.add(
                                (in_fasta_path, hmm_path,
                                    out_result_path, "ga"))

                print("Running hmmscans in parallel...")

                # do hmmscans in parallel
                hmmscan_results = pool.map(
                    run_hmmscan, hmmscan_queues)

                print("Parsing hmmscans results...")
                # parse hmmscan txts and create database
                # entries for them, then update bgc_status
                hmm_ids = {
                    hmm.accession: hmm.id for hmm in hmm_db.biosyn_pfams}

                for chunk, chunk_name in get_chunk(
                        to_be_hmmscanned, hmmscan_chunk_size):

                    in_fasta_path = fasta_path(chunk_name)
                    out_result_path = hmm_result_path(chunk_name)

                    for hsp_object in HSP.parse_hmmtext(
                            out_result_path, hmm_ids):
                        hsp_object.save(output_db)

                    output_db.commit_inserts()

                    for bgc_id in chunk:
                        if update_bgc_status(
                                bgc_id, run.id, 2, output_db) == 1:
                            hmm_scanned.add(bgc_id)
                        else:
                            print("Failed to update bgc status " +
                                  "(bgc_id: " + str(bgc_id) + ") " +
                                  "(run_id: " + str(run.id) + ")")

                    # clean up
                    remove(in_fasta_path)
                    remove(out_result_path)

                del hmm_ids

            # update status bin
            for bgc_id in hmm_scanned:
                assert run.bgcs[bgc_id] == 1
                run.bgcs[bgc_id] = 2
            bgc_status_bin[1] = bgc_status_bin[1] - hmm_scanned
            bgc_status_bin[2].update(hmm_scanned)

            if len(to_be_hmmscanned) > 0 and use_memory:
                output_db.dump_db_file()

            # garbage collection
            del to_be_hmmscanned
            del biosyn_tmp_folder
            del fasta_path
            del hmm_result_path

            print("[{}s] biosyn_pfam scan".format(get_elapsed()))
        if len(bgc_status_bin[1]) == 0:
            if run.status < 2:
                if run.update_status(2) > 0:
                    print("run_status is now BIOSYN_SCANNED")
                else:
                    print("failed to update run_status")
        else:
            print("Encountered errors in hmmscans, " +
                  str(len(bgc_status_bin[1])) +
                  " BGCs are not yet scanned.")
            return 1

        # phase 3: Subpfam scanning
        if len(bgc_status_bin[2]) > 0:
            get_elapsed()
            print("Doing sub_pfam scan on {} BGCs...".format(
                len(bgc_status_bin[2])))

            subpfam_scanned = set()

            # check if already subpfam_scanned in previous run
            for chunk, chunk_name in get_chunk(list(bgc_status_bin[2]), 1000):
                for bgc_id in check_subpfam_scanned_in_db(
                        chunk, md5_sub_pfam, output_db):
                    if update_bgc_status(bgc_id, run.id, 3, output_db) == 1:
                        subpfam_scanned.add(bgc_id)
                    else:
                        print("Failed to update bgc status " +
                              "(bgc_id: " + str(bgc_id) + ") " +
                              "(run_id: " + str(run.id) + ")")
                        return 1

            print("{} BGCs are already scanned in previous run".format(
                len(subpfam_scanned)))

            # prepare for subpfam_scan
            subpfam_tmp_folder = path.join(tmp_folder, md5_sub_pfam)
            if not path.exists(subpfam_tmp_folder):
                makedirs(subpfam_tmp_folder)

            def fasta_path(bgcs_md5, hmm_id):
                return path.join(
                    subpfam_tmp_folder, "bgc_{}_hmm_{}.fa".format(
                        bgcs_md5, hmm_id))

            def hmm_result_path(bgcs_md5, hmm_id):
                return path.join(
                    subpfam_tmp_folder, "bgc_{}_hmm_{}.hmmtxt".format(
                        bgcs_md5, hmm_id))

            # fetch core pfam ids
            core_pfam_ids = {}
            core_pfam_accs = {}
            for parent_hmm_acc in hmm_db.sub_pfams:
                for hmm_obj in hmm_db.biosyn_pfams:
                    if hmm_obj.accession == parent_hmm_acc:
                        core_pfam_ids[hmm_obj.accession] = hmm_obj.id
                        core_pfam_accs[hmm_obj.id] = hmm_obj.accession
                        break

            to_be_subpfam_scanned = list(bgc_status_bin[2] - subpfam_scanned)
            if len(to_be_subpfam_scanned) > 0:

                chunks_and_pfam_ids = {}

                print("Preparing fasta files for subpfam_scans...")
                hmmscan_queues = set()
                for chunk, chunk_name in get_chunk(
                        to_be_subpfam_scanned, subpfam_chunk_size):

                    fasta_sequences = BGC.get_all_aligned_hsp(
                        chunk, core_pfam_ids.values(), output_db)
                    chunks_and_pfam_ids[chunk_name] = set(
                        fasta_sequences.keys())

                    for parent_hmm_id in fasta_sequences:
                        in_fasta_path = fasta_path(
                            chunk_name, parent_hmm_id)
                        hmm_path = path.join(program_db_folder,
                                             "sub_pfams",
                                             "hmm",
                                             core_pfam_accs[
                                                 parent_hmm_id] +
                                             ".subpfams.hmm")
                        out_result_path = hmm_result_path(
                            chunk_name, parent_hmm_id)

                        if not path.exists(in_fasta_path):
                            with open(in_fasta_path, "w") as fa:
                                fa.write(fasta_sequences[parent_hmm_id])

                        hmmscan_queues.add(
                            # for subpfam_scan: threshold == 20 bitscores
                            (in_fasta_path, hmm_path, out_result_path, 20))

                # do hmmscans in parallel
                print("Running subpfam_scans in parallel...")
                hmmscan_results = pool.map(
                    run_hmmscan, hmmscan_queues)

                print("Parsing subpfam_scans results...")
                for chunk, chunk_name in get_chunk(
                        to_be_subpfam_scanned, subpfam_chunk_size):
                    chunk_parent_pfams = chunks_and_pfam_ids[chunk_name]
                    for parent_hmm_id in chunk_parent_pfams:
                        out_result_path = hmm_result_path(
                            chunk_name, parent_hmm_id)
                        sub_pfam_ids = {
                            hmm_obj.name: hmm_obj.id
                            for hmm_obj in
                            hmm_db.sub_pfams[core_pfam_accs[parent_hmm_id]]
                        }

                        for hsp_object in HSP.parse_hmmtext(
                                out_result_path, sub_pfam_ids,
                                save_alignment=False):
                            hsp_object.save(output_db)

                    output_db.commit_inserts()

                    for bgc_id in chunk:
                        if update_bgc_status(
                                bgc_id, run.id, 3, output_db) == 1:
                            subpfam_scanned.add(bgc_id)
                        else:
                            print("Failed to update bgc status " +
                                  "(bgc_id: " + str(bgc_id) + ") " +
                                  "(run_id: " + str(run.id) + ")")

                    # clean up
                    for parent_hmm_id in chunk_parent_pfams:
                        remove(fasta_path(
                            chunk_name, parent_hmm_id))
                        remove(hmm_result_path(
                            chunk_name, parent_hmm_id))

            # update status bin
            for bgc_id in subpfam_scanned:
                assert run.bgcs[bgc_id] == 2
                run.bgcs[bgc_id] = 3
            bgc_status_bin[2] = bgc_status_bin[1] - subpfam_scanned
            bgc_status_bin[3].update(subpfam_scanned)

            print("[{}s] sub_pfam scan".format(get_elapsed()))

            if len(to_be_subpfam_scanned) > 0 and use_memory:
                output_db.dump_db_file()

        if len(bgc_status_bin[2]) == 0:
            if run.status < 3:
                if run.update_status(3) > 0:
                    print("run_status is now SUBPFAM_SCANNED")
                else:
                    print("failed to update run_status")
        else:
            print("Encountered errors in hmmscans, " +
                  str(len(bgc_status_bin[2])) +
                  " BGCs are not yet scanned.")
            return 1

        # phase 4: Features extraction
        if 2 < run.status < 4:
            get_elapsed()
            print("Extracting features from {} BGCs...".format(len(run.bgcs)))

            if not path.exists(features_folder):
                makedirs(features_folder)

            features = Features.extract(
                run.id, "default", output_db, pool
            )
            features.save(features_folder)
            del features

            print("[{}s] features extraction".format(get_elapsed()))
            update_bgcs_status = output_db.update("run_bgc_status",
                                                  {"status": 4},
                                                  "WHERE run_id=?",
                                                  (run.id,))
            if update_bgcs_status > 0 and run.update_status(4) > 0:
                print("run_status is now FEATURES_EXTRACTED")
            else:
                print("failed to update run_status")

        # phase 5: Clustering
        if 3 < run.status < 5:
            get_elapsed()
            print("Clustering {} BGCs...".format(len(run.bgcs)))

            if not path.exists(centroids_folder):
                makedirs(centroids_folder)

            # fetch mibig bgc ids as reference bgc ids
            # todo: use datasets
            reference_bgc_ids = [row["id"] for row in output_db.select(
                "bgc",
                "WHERE type LIKE 'mibig'",
                props=["id"]
            )]

            clustering = BirchClustering.run(
                run.id,
                features_folder,
                output_db,
                reference_bgc_ids
            )
            clustering.save(centroids_folder)
            del clustering

            print("[{}s] clustering".format(get_elapsed()))
            update_bgcs_status = output_db.update("run_bgc_status",
                                                  {"status": 5},
                                                  "WHERE run_id=?",
                                                  (run.id,))
            if update_bgcs_status > 0 and run.update_status(5) > 0:
                print("run_status is now CLUSTERING_FINISHED")
            else:
                print("failed to update run_status")

        # phase 6: finishing, preparing outputs
        exit(1)  # TODO: implements
        if run.status < 6:
            run.status == 6
            run.save()

    # exit program cleanly
    return 0


if __name__ == "__main__":
    exit(main())
